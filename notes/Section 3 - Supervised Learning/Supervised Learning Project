--------------------------------- Project  (Major learning points) -------------------------------------
  Highlights:
  A. Prep / Preprocess Data:
     1. See the data carefully.

     2. Normalize Skewed data:
        Find out the highly skewed columns - logarithmically transform to reduce range of values for better performance
        by the learning algorithm

     3. Normalize Numerical features:
        Find out features that have 'categorical' data (lookups in db terms). Transform those string lookups to integral
        values for better performance by the learning algorithms. Typically, this is done via 'One-hot encoding' scheme.
        This is also termed as 'applying scaling' on the data. See pd.get_dummies() on how columns are added
        automatically based on the distribution on the fields you normalized.

     4. Just like with feature - we should normalized the target variable too. So instead of strings like '>50K' and
        '<=50K', we use 1 and 0. (note, we do not have to use get_dummies for this simpler normalization. Just replace
        the strings with integers and throw it over to a variable.

   B. Evaluated Model Performance (picked the following three in addition to the default 'Naive Predictor':
       1. Naive Predictor

       2. Decision Trees.
       3. K-Nearest Neighbour.
       4. Support Vector Machines.

  Details (see also notes from previous section on accuracy, precision and recall):
    Compute the size of a dataframe
      --> len(data.index)
    Compute the records with condition
      rows_with_n_greater_than_50k = data.apply(lambda x: True if x['income'].lower() == '>50k' else False , axis=1)
      n_greater_50k = len(rows_with_n_greater_than_50k[rows_with_n_greater_than_50k == True].index)

    Highly skewed features must be normalized or else the learning algorithms won't perform well.
    =>skewed --> columns with data spread all over a huge range but concentrated mainly around famous 'values'. Use the
    famous values rather than the original values.

    This 'normalization' is done through a famous technique 'logarithmic transformation'
    income_raw = data['income']
    features_raw = data.drop('income', axis = 1)

    #Visualize the original 'data' here to see the value (x-axis) spreads around thousands of values for two columns
    #capital-gain and capital-loss.  This make the learning algos slow.
    vs.distribution(data)

    # Side Question on the above function:
    Is it feature of this 'distribution()'' function that prints just the skewed columns? There is no
    mention or input to this function that tell is to print these two columns. So it must be  'intelligent'
    enough to consider just the graphs for the skewed columns only.

    skewed = ['capital-gain','capital-loss']
    features_log_transformed = pd.Dataframe(features_raw)
    features_log_transformed [skewed] = features_raw[skewed].apply(lambda x: np.log(x+1))

    #visualize again now and see how the values (x-axis) are spread only in 10's instead of thousands.
    vs.distribution(features_log_transformed, transformed = True)

    # Side Question ... hmm, see the 'Transformed = True' parameter to the distribution function. Interesting !!!

    Question1:
         For Naive base, the model always predicts 1 (makes >=50K). So there are no Negatives (false or true).
         Accuracy and precision are the same ... and Recall is always 1.

         Recall the formulas.
         a. Accuracy --> "How often our model makes the correct prediction."
            Accuracy = (True Positive + True Negative) / Total

         b. Precision --> "What portion of messages identified as spam were actually spam"?
            Precision = True Positive / Actual results
            OR => True Positive / (True Positive + False Positive)

         c. Recall --> "What portion of messages that were actually spam WERE actually identified by us as spam?"
            Recall = True Positive / Predicted Results
            OR => True Positive / (True Positive + False Negative)

         + ------------------ + ------------------------------------- +
         |          \ Reality | Actually          | Actually          |
	     |           \        | Made              | doesn't make      |
	     | Prediction \       | >50K              | >50K              |
	     + ------------------ + ----------------- + ----------------- +
	     | Predicted          |                   |                   |
	     | Makes >50K         |        0          |       10          | --> 10 predicted to be non-spam
	     |                    |                   |                   |     so all are False Positive
	     + ------------------ + ----------------- + ----------------- +
	     | Predicted doesn't  |                   |                   |
	     | make >50K          |        0          |       0           | --> 90 predicted as non-spam, including
	     |                    |                   |                   |     2 that were spam (and False Negative)
	     + ------------------ + ----------------- + ----------------- +
	                                    |                   |
	                                    V                   V
	                             Actual >50K         Actual Not >50K

    Question2: Supervised machine learning models.
     Good Articles: https://books.google.com/books?hl=en&lr=&id=vLiTXDHr_sYC&oi=fnd&pg=PA3&dq=supervised+machine+learning&ots=CYrzutYIkm&sig=cNVv6QKZChFta3YjleaPjzrzQBs#v=onepage&q=supervised%20machine%20learning&f=true
      Supervised Machine Learning:
        -- You know what features to train the data on.
        -- Generally, the data is prepared by the expert and the features the training is needed on are mentioned to the
           classifier.

           What can go wrong with the data?

           1. Noise: when come instances of the training data do not have the expected values.
               -- outlier (noise) detection - some area of research.

           2. Missing values. Researchers seem to be busy dealing with this issue too .. hmm. But basically, it means
              that the data for some features are totally missing for some instances (rows) of the training data.

           3. The are features in the data that are irrelevant. Some folks are busy doing research in this area too to
              draw meanings or new features from the existing non-relevant features.

      Gaussian Naive Bayes (GaussianNB)
      Decision Trees:
        Good Video: https://www.youtube.com/watch?v=7VeUPuFGJHk
      Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)
      K-Nearest Neighbors (K-Neighbors)
      Stochastic Gradient Descent Classifier (SGDC)
      Support Vector Machines (SVM)
      Logistic Regression
