Bagging:
  1. Train multiple 'weak' learners on separate parts of the data.
  2. Combine all trainers's output to get one stronger learner.

Boosting (Based on Adaboost algorithm):
  1. Assign weights to each learner's points (correctly predicted vs incorrectly predicted ones points)
  2. Combine the learners to get one stronger learner
  3. To assign weights to good and bad learners, ln() is the best function. Note, a learner that always tells the truth
     is as good as the one that always lies (because we can do the reverse of that - the worse ones are those that lie
     half the time and tell truth the other half).

         y = ln (accuracy/1-accuracy) or
         y = ln (#correct points)/(#incorrect points)

                                  y

                 |                |              x | (Good Predictors)
                 |                |              x |  #Always say the truth
                 |                |              x |
                 |                |              x |
                 |                |             x  |
                 |                |           x    |
                 |                |       x        |
                 | -1           0 |   x            | 1
       ----------|----------------x----------------|------ x
                 |               x|(Bad Predictors)|
                 |           x    | #Half Truth &  |
                 |       x        | #Half lies     |
                 |   x            |                |
                 | x              |                |
                 |x               |                |
                 |x               |                |
                 |x               |                |

             ('Good' Predictors)
             #Always say the lie - but we do the inverse of it.


