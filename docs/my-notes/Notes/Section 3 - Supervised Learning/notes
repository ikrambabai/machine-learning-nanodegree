     Predictive Machine Learning
       /                    \
	  /                      \
	 /                        \
	/                          \ 
  clasification              regression
(quetions of form yes/no)    (questions of form: how much/how many etc.)


Linear Regression:
  y = w1x + w2
    Data Points want the line come close to it.
    Move line up and down (play with w2)
    Rotate line (plan with w1)
	for a point (p, q) use 
	y = (w1+p)x + w2+1 (we added 1 to the y-intercept and p to the slope)
	
	The reason p is there - is very useful - for negative numbers (number in 4th quardant e.g), the line rotates in clock wise (reverse) instead of anti-clock wise. powerful.
	
  Absolute trick: 
	But this may endup in making big steps
	Use Leanring rate, alpha
	Instead of adding 1 to the y-intercept and p to the slope, we muliple w1 with alpha and add it to w2
	y = (w1 + p*aplha)x + w2+1*aplha
  
  Square Trick - with extra gravy
    Add q to the above discussion too. 
    So for point (p, q) - the vertical distnace from this point to the line given the line point is (p, q') is q-q'. Multiply this to the alpha in above formula. 
	y = (w1 + p*aplha*(q-q'))x + w2+1*aplha*(q-q')
  
  Minmize Mean Absolute Error 
  Minmize Mean Square Error

  Gradient Descent:
     wi = wi - derivate of error w.r.t i?
	 Summary, minimizing MSE or MSE with Gradient descent is same as Absolute and Square trick.
     Practice !!!	 
  
  See work on derivates in notebook on how to derivate Error w.r.t w1 and w2 where y = w1.x + w2 and Error = 1/2 square(y-y')
  
  MAE (or Mean Total Error) vs MSE (Mean Squared Error): 
    Three lines passing through 4 data points.
	Which one of the 3 lines will give lowest value for MAE?
	   All three give same error.
	   
	Which one of the 3 lines will give lowest value for MSE?
	   Line in the middle gives least MSE.
   
  "Fit the model" means picking up the solution (line in our case) that has the least of errrors.
     Example:
	 
 	    import pandas as pd
		from sklearn.linear_model import LinearRegression

		bmi_life_data = pd.read_csv("bmi_and_life_expectancy.csv")

		bmi_life_model = LinearRegression()
		bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])

		laos_life_exp = bmi_life_model.predict(21.07931)
		
		about 60 years prediction.
		
  Higher Dimentions (Line Vs. Plane):
    So far our linear regression equation has been a LINE ... we had an input variable, and an output variable (y = w1.x + w2). 
	But what if we had two inputs to the function? It will be 3-dimentional space - a line won't do - we have to draw a PLAN rather than a line because our points are 
	lying in a three-dimentional space now.
	
	so z will now be w1.x + w2.y + w3
	
	So for n-dimentional plane,
	
	Summary: our prediction will be an n-1 dimentoinal hyperplan sittin in n-dimentional space.
	Hard to think of n-dimentional space - just think of a linear equation and n variables.
		y' = w1.x1 + w2.x2 + w3.x3 ... wn
	
	So we do exact same thing we did we two variable dimention. Find out all w's with whatever your trick (mean or absolute) or use MSE or MAE - and minimize using Gradient Descent

  (Optional) Closed form solution: Please do it at a better time

  Polynomial Regression: Non-linear regression - i.e. cases when a model cannot fit the data using a straing line but rather a curved or wavy line. In these
    cases we use higher degree polynomials - but use the same techniques
    for example: y' = w1. x^3 + w2. x^2 + w3.x + w4
	But technique is the same.
	  0). We take the MSE or MAE and
	  1). Derivate w.r.t to all 4 weights (ws) and
	  2). Gradient Descent to use these 4 weights to minimize the error.
	  
  L1 and L2:
     Summary: Punish the more fitting and more complex (even though it may have less error) by addding the 'complexity' to the error function ... for example
	 adding the absolute values of the coefficeints of the terms used (except the constants).
	 
	 Summary2: Steer the level of 'punishment' through a multiplier lambda according to your situation.
	 
	 
-------- Decision Trees --------------

Perception Algorithms
    Talking strictly about Classification problems now.
	Classification --> yes/no type questions
	Term: we can 'eyeball' it (if a student passed/failed) but computer cannot.
 
  Practiced AND, OR  and the elusive XOR neaural networks on papper.
  
  Which feature describes the data baset. See drawings in notebook
  See quizes done in book
  Decisions trees can aswer questions of yes/no as well as threshbold based questions ... see done in notebook.

  Entropy: - p1*log_base2(p1) - p2*log_base2(p2) - .... pn*log_base2(pn)  	
    Example:
      8 red, 3 blue, 2 yellow balls entropy will be -8.0/13*math.log(8.0/13, 2) - 3.0/13*math.log(3.0/13, 2) - 2.0/13 * math.log(2.0/13, 2) 



Large depth very often causes overfitting, since a tree that is too deep, can memorize the data. Small depth can result in a very simple model, which may cause underfitting.
Small minimum samples per split may result in a complicated, highly branched tree, which can mean the model has memorized the data, or in other words, overfit. Large minimum samples may result in the tree not having enough flexibility to get built, and may result in underfitting.  
  
  

	
	

  
 	
	
	
 